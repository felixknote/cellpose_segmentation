{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3147967-cc24-478b-b1b9-d8a9c8e4f2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Welcome to CellposeSAM, cellpose v\n",
      "cellpose version: \t4.0.6 \n",
      "platform:       \twin32 \n",
      "python version: \t3.8.20 \n",
      "torch version:  \t2.4.0! The neural network component of\n",
      "CPSAM is much larger than in previous versions and CPU excution is slow. \n",
      "We encourage users to use GPU/MPS if available. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import pandas as pd\n",
    "import pkg_resources\n",
    "from cellpose import models\n",
    "from skimage.measure import regionprops, label\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a3a0c6-94ec-4317-a41a-29c943cb5198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "4.0.6\n",
      "GPU Name: NVIDIA RTX 4500 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA and GPU availability\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "print(pkg_resources.get_distribution(\"cellpose\").version)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32315f14-1622-43c7-ba16-fecca4eff04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS - update these paths\n",
    "input_folder = r\"L:\\43-RVZ\\AIMicroscopy\\Mitarbeiter\\2_Data\\1_NikonTi2\\2025_07_29 60xWI 1.5x nanobody titration\\TIF\\stacked\"\n",
    "output_folder = r\"L:\\43-RVZ\\AIMicroscopy\\Mitarbeiter\\2_Data\\1_NikonTi2\\2025_07_29 60xWI 1.5x nanobody titration\\TIF\\stacked\\segmentation\\cellposeSAM_gpu_results\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f59229-3ffe-474d-9627-731738db5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new Cellpose-SAM model is loaded via model_type = 'sam'\n",
    "model = models.CellposeModel(gpu=torch.cuda.is_available(), pretrained_model='cpsam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e976944-9108-4670-9969-d87066e5a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch using 36 threads\n"
     ]
    }
   ],
   "source": [
    "# Segmentation parameters\n",
    "torch.set_num_threads(36)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"36\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"36\"\n",
    "\n",
    "print(f\"PyTorch using {torch.get_num_threads()} threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae4bbe4-3ec1-49ee-8d1e-ff5ef385f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diameter = None  # cell diameter; set to None for automatic\n",
    "batch_size = 128\n",
    "tile_overlap = 0.05\n",
    "bsize = 256  # tile size for Cellpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b676e75e-b4ef-4268-b256-135014ed4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PyTorch environment variable to reduce CUDA memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"# Utility function: shape features from regionprops\n",
    "def shape_features(region):\n",
    "    area = region.area\n",
    "    perimeter = region.perimeter if region.perimeter > 0 else 1\n",
    "    roundness = 4 * np.pi * area / (perimeter ** 2)\n",
    "    major_axis = region.major_axis_length\n",
    "    minor_axis = region.minor_axis_length\n",
    "    aspect_ratio = major_axis / minor_axis if minor_axis != 0 else 0\n",
    "    return {\n",
    "        \"Area\": area,\n",
    "        \"Length\": major_axis,\n",
    "        \"Width\": minor_axis,\n",
    "        \"Roundness\": roundness,\n",
    "        \"Aspect Ratio\": aspect_ratio,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a54013-7242-4f94-b621-46ce674f342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_stack(stack_path):\n",
    "    stack = tifffile.imread(stack_path)\n",
    "\n",
    "    # Expect shape (2, H, W): DIC and fluorescence\n",
    "    if stack.ndim == 3 and stack.shape[0] == 2:\n",
    "        dic_img = stack[0]\n",
    "        fluo_img = stack[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected image shape {stack.shape} in {stack_path}\")\n",
    "\n",
    "    # Normalize each image independently\n",
    "    def normalize_channel(img):\n",
    "        mn, mx = img.min(), img.max()\n",
    "        return (img - mn) / (mx - mn) if mx > mn else img\n",
    "\n",
    "    dic_img = normalize_channel(dic_img)\n",
    "    #fluo_img = normalize_channel(fluo_img)\n",
    "\n",
    "    # Prepare input for Cellpose-SAM: (channels, H, W)\n",
    "    img_cp = np.expand_dims(dic_img, axis=0)\n",
    "\n",
    "    # Pad to multiple of tile size\n",
    "    def pad_to_multiple(img, multiple):\n",
    "        c, h, w = img.shape\n",
    "        pad_h = (multiple - h % multiple) % multiple\n",
    "        pad_w = (multiple - w % multiple) % multiple\n",
    "        img_padded = np.pad(img, ((0, 0), (0, pad_h), (0, pad_w)), mode='reflect')\n",
    "        return img_padded, (h, w)  # original height and width\n",
    "\n",
    "    img_cp, orig_shape = pad_to_multiple(img_cp, bsize)\n",
    "\n",
    "    # Run Cellpose-SAM model\n",
    "    masks_padded, flows, styles = model.eval(\n",
    "        img_cp,\n",
    "        diameter=diameter,\n",
    "        batch_size=batch_size,\n",
    "        augment=False,\n",
    "        #Turn this to true if you want better quality segmentation\n",
    "        tile_overlap=tile_overlap,\n",
    "        bsize=bsize,\n",
    "    )\n",
    "\n",
    "    # Crop masks back to original shape, handle 2D or 3D masks output\n",
    "    if masks_padded.ndim == 3:  # (batch, H, W)\n",
    "        masks = masks_padded[0][:orig_shape[0], :orig_shape[1]]\n",
    "    elif masks_padded.ndim == 2:  # (H, W) no batch dim\n",
    "        masks = masks_padded[:orig_shape[0], :orig_shape[1]]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected masks_padded shape: {masks_padded.shape}\")\n",
    "\n",
    "    # Sanity check\n",
    "    if masks.shape != dic_img.shape:\n",
    "        raise ValueError(f\"Mismatch after cropping: masks shape {masks.shape} vs dic_img {dic_img.shape}\")\n",
    "\n",
    "    # Label mask for regionprops analysis\n",
    "    labeled_masks = label(masks)\n",
    "    props = regionprops(labeled_masks, intensity_image=fluo_img)\n",
    "\n",
    "    # Background intensity\n",
    "    background_mask = masks == 0\n",
    "    background_values = fluo_img[background_mask]\n",
    "    background_mean = float(np.mean(background_values)) if background_values.size > 0 else 0\n",
    "    background_std = float(np.std(background_values)) if background_values.size > 0 else 0\n",
    "\n",
    "    # Extract features\n",
    "    data = []\n",
    "    for prop in props:\n",
    "        feats = shape_features(prop)\n",
    "        data.append({\n",
    "            \"Label\": prop.label,\n",
    "            \"Area\": feats[\"Area\"],\n",
    "            \"Length\": feats[\"Length\"],\n",
    "            \"Width\": feats[\"Width\"],\n",
    "            \"Roundness\": feats[\"Roundness\"],\n",
    "            \"Aspect Ratio\": feats[\"Aspect Ratio\"],\n",
    "            \"Mean Intensity\": prop.mean_intensity,\n",
    "            \"Centroid X\": prop.centroid[1],\n",
    "            \"Centroid Y\": prop.centroid[0],\n",
    "            \"Background Mean\": background_mean,\n",
    "            \"Background Std\": background_std,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df, masks.astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e6b1a8-d798-435c-a9e0-d2704fd392a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory check and parameter adjustment for OOM errors\n",
    "def get_free_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    return torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "\n",
    "def adjust_parameters_on_oom(current_batch, current_bsize):\n",
    "    free_mem = get_free_gpu_memory()\n",
    "    print(f\"Free GPU memory: {free_mem / (1024**3):.2f} GB\")\n",
    "    if free_mem < 5 * (1024**3):\n",
    "        new_batch = max(64, current_batch // 2)\n",
    "        new_bsize = max(128, current_bsize // 2)\n",
    "        print(f\"Reducing batch size from {current_batch} to {new_batch} and tile size from {current_bsize} to {new_bsize}\")\n",
    "        return new_batch, new_bsize\n",
    "    return current_batch, current_bsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f786ca98-c53b-46e4-98aa-6ce1b24d25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_stack_with_retries(stack_path, retries=3):\n",
    "    global batch_size, bsize\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return analyze_single_stack(stack_path)\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"⚠️ CUDA OOM on attempt {attempt + 1} for {os.path.basename(stack_path)}. Adjusting parameters...\")\n",
    "                batch_size, bsize = adjust_parameters_on_oom(batch_size, bsize)\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                raise\n",
    "    raise RuntimeError(f\"Failed to process {stack_path} after {retries} attempts due to OOM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71449cd9-7535-429f-a1a7-b2e03a3fdd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 93 TIFF stacks for analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TIFF stacks: 100%|███████████████████████████████████████████████████████████████████████| 93/93 [18:15<00:00, 11.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch analysis complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main batch analysis loop with progress bar\n",
    "tiff_files = [f for f in os.listdir(input_folder) if f.lower().endswith((\".tif\", \".tiff\"))]\n",
    "print(f\"Found {len(tiff_files)} TIFF stacks for analysis.\")\n",
    "\n",
    "with tqdm(total=len(tiff_files), desc=\"Processing TIFF stacks\", dynamic_ncols=True) as pbar:\n",
    "    for fname in tiff_files:\n",
    "        fpath = os.path.join(input_folder, fname)\n",
    "        try:\n",
    "            df, masks = analyze_single_stack_with_retries(fpath)\n",
    "\n",
    "            excel_path = os.path.join(output_folder, f\"{os.path.splitext(fname)[0]}_results.xlsx\")\n",
    "            df.to_excel(excel_path, index=False)\n",
    "\n",
    "            masks_path = os.path.join(output_folder, f\"{os.path.splitext(fname)[0]}_masks.tif\")\n",
    "            tifffile.imwrite(masks_path, masks.astype(np.uint16))\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {fname}: {e}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"✅ Batch analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
